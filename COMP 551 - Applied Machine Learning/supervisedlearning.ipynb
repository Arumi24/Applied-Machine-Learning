{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comp 551-Assignment 3\n",
    "\n",
    "\n",
    "## Aymen Rumi-260661663\n",
    "   \n",
    "### ***-Refer to report for further detailed exlanation/observation/answers to each inidivudal question and their subquestions: this file contains mainly code and ouput values that have taken a very long time to run-***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import csv\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "import itertools\n",
    "from scipy import sparse\n",
    "import re\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Convert Dataset & Create Vocab List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below contains all method used to convert text file into, vocab table, translation into numerical table, conversion to binary bag of words representation, and conversion to frequency bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# function given text file,removes all punctuation, uppercase\n",
    "# and returns an array value of the word with the number id and frequency in the text\n",
    "def vocabulary_list(csv_file, cutoff):  \n",
    "    # initializing punctuations and various other characters for removal\n",
    "    punctuations = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "    removals = re.compile(r'<[^>]+>')\n",
    "    temporary = Counter()    \n",
    "    #iterate through each row in csv file, each row corresponds to a review\n",
    "    for index,row in csv_file.iterrows():\n",
    "        # change all uppercase to lowercase, remove punctuation and remove non-ascii characters     \n",
    "        review = row[0].lower().translate(punctuations).encode('ascii', errors='ignore').decode()\n",
    "        # remove HTML tags\n",
    "        review = removals.sub('', review)\n",
    "        count = Counter(review.split())\n",
    "        temporary = temporary + count\n",
    "    table = []\n",
    "    counter = 1\n",
    "    #return a table for sorted vocabulary words for up tp cutoff number\n",
    "    for word, value in sorted(temporary.items(), key=itemgetter(1), reverse=True):\n",
    "        table.append((word, counter, value))\n",
    "        counter = counter+1\n",
    "        if counter > cutoff:\n",
    "            return table\n",
    "    while counter <= cutoff:\n",
    "        table.append((\"0\", counter, 0))\n",
    "        counter += 1\n",
    "    return table\n",
    "\n",
    "\n",
    "def column(array, col):\n",
    "        return [row[col] for row in array]\n",
    "\n",
    "#given a text file, it replaces all the words in the review with it's numerical correspondance\n",
    "def text_translation(csv_file, vocab):\n",
    "    # initializing punctuations and various other characters for removal\n",
    "    punctuations = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "    printable_table = str.maketrans(dict.fromkeys(string.printable))\n",
    "    removals = re.compile(r'<[^>]+>')\n",
    "    words = column(vocab, 0)\n",
    "    numbers = column(vocab, 1)\n",
    "    for index, row in csv_file.iterrows():       \n",
    "        review = row[0].lower().translate(punctuations)\n",
    "        review_copy = review\n",
    "        for word in review_copy.split(' '):\n",
    "            try:\n",
    "                review = re.sub(r\"\\b%s\\b\" % word, str(numbers[words.index(word)]), review)\n",
    "            except ValueError:\n",
    "                review = re.sub(r\"\\b%s\\b\" % word, \"\", review)\n",
    "        csv_file.iat[index, 0] = review\n",
    "    return csv_file\n",
    "\n",
    "\n",
    "def makeFile(fileName, data):\n",
    "    file = open(fileName, 'w')   \n",
    "    for line in data:\n",
    "        file.write('\\t'.join(str(s) for s in line) + '\\n')                          \n",
    "    file.close()   \n",
    "    return file\n",
    "\n",
    "def makeTranslation(file,vocabFile, targetFile, vocabulary):\n",
    "    vocab=open(vocabFile, 'r')\n",
    "    vocab_table=[]\n",
    "    with open(vocabFile) as input:\n",
    "        for line in input:\n",
    "            temp = line.split('\\t')\n",
    "            vocab_table.append([temp[0], int(temp[1]), int(temp[2])])\n",
    "    pd.DataFrame.to_csv(text_translation(file, vocabulary), path_or_buf=targetFile, sep='\\t', header=None, index=False)\n",
    "  \n",
    "    \n",
    "def frequency_bag_of_words(data,size):\n",
    "    change = sparse.lil_matrix((data.shape[0], size))\n",
    "    for index, row in data.iterrows():\n",
    "        if type(row[0]) == type('str'):\n",
    "            split = row[0].split(' ')\n",
    "            for number in split:\n",
    "                try:\n",
    "                    change[index, int(number) - 1] += (1/len(split))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "    return change.tocsr() \n",
    "\n",
    "def binary_bag_of_words(data,size): \n",
    "    change = sparse.lil_matrix((data.shape[0], size))\n",
    "    for index, row in data.iterrows():\n",
    "        if type(row[0]) == type('str'):\n",
    "            for number in row[0].split(' '):\n",
    "                try:\n",
    "                    change[index, int(number) - 1] = 1\n",
    "                except ValueError:\n",
    "                    pass\n",
    "    return change.tocsr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below code initalizes all the datasets used and creates vocabulary list and tranlsates text files for yelp reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "yelp_train=pd.read_csv(\"yelp-train.txt\", sep='\\t', header=None)\n",
    "yelp_test=pd.read_csv(\"yelp-test.txt\", sep='\\t', header=None)\n",
    "yelp_valid=pd.read_csv(\"yelp-valid.txt\", sep='\\t', header=None)\n",
    "\n",
    "\n",
    "yelp_vocab = vocabulary_list(yelp_train,10000)\n",
    "yelp_vocab_file = makeFile(\"yelp-vocab.txt\",yelp_vocab)\n",
    "\n",
    "makeTranslation(yelp_train,\"yelp-vocab.txt\",\"yelp-train.txt\",yelp_vocab)\n",
    "makeTranslation(yelp_test,\"yelp-vocab.txt\",\"yelp-test.txt\",yelp_vocab)\n",
    "makeTranslation(yelp_valid,\"yelp-vocab.txt\",\"yelp-valid.txt\",yelp_vocab)\n",
    "\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below code initalizes binary bag of words and frequency bag of words for yelp review text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "yelp_train=pd.read_csv(\"yelp-train.txt\", sep='\\t', header=None, encoding='ISO-8859-1', dtype=None)\n",
    "yelp_test=pd.read_csv(\"yelp-test.txt\", sep='\\t', header=None, encoding='ISO-8859-1', dtype=None)\n",
    "yelp_valid=pd.read_csv(\"yelp-valid.txt\", sep='\\t', header=None, encoding='ISO-8859-1', dtype=None)\n",
    "\n",
    "binary_yelp_train=binary_bag_of_words(yelp_train,10000)\n",
    "binary_yelp_test=binary_bag_of_words(yelp_test,10000)\n",
    "binary_yelp_valid=binary_bag_of_words(yelp_valid,10000)\n",
    "\n",
    "frequency_yelp_train=frequency_bag_of_words(yelp_train,10000)\n",
    "frequency_yelp_test=frequency_bag_of_words(yelp_test,10000)\n",
    "frequency_yelp_valid=frequency_bag_of_words(yelp_valid,10000)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A method, given a model and parameters, outputs the model with the best parammeter and it's F score for valid, it is trained on the training set\n",
    "\n",
    "## Contains Model training for Naive Baiyes, Decision Trees, Linear Support Vector Machine, and other non specific classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a method given a model and specified model used, runs the model with a list of paramters and outputs best f1 measure\n",
    "def model_training(model, train, train_cols, valid, valid_cols, test, test_cols,\n",
    "                   parameters=None, model_used=\"\"):\n",
    "    # when linear svm, naieve bayes, or decision tree isnt specified\n",
    "    if model != None or parameters == None:\n",
    "        model.fit(train, train_cols)\n",
    "        prediction = pd.DataFrame(model.predict(train)).astype(int)\n",
    "        print (model) \n",
    "        print(\"_________________________________________\")     \n",
    "        print(\"F1 Score(Train): \", f1_score(train_cols.astype(int), prediction, average='micro'))      \n",
    "        prediction = pd.DataFrame(model.predict(valid)).astype(int)\n",
    "        print(\"F1 Score(Valid): \", f1_score(valid_cols.astype(int), prediction, average='micro')) \n",
    "        prediction = pd.DataFrame(model.predict(test)).astype(int)\n",
    "        print(\"F1 Score(Test): \", f1_score(test_cols.astype(int), prediction, average='micro'))\n",
    "        print(\"\\n\")\n",
    "        return model\n",
    "    output_model = None\n",
    "    output_f1 = -1\n",
    "    \n",
    "    # when the model is naieves bayes\n",
    "    if model_used == \"BernoulliNaiveBayes\" or model_used == \"MultinomialNaiveBayes\":\n",
    "\n",
    "        for alpha in parameters.get('alpha'):\n",
    "            for fit_prior in parameters.get('fit_prior'):\n",
    "                if model_used == \"BernoulliNaiveBayes\":\n",
    "                    model = BernoulliNB(alpha=alpha, fit_prior=fit_prior)\n",
    "                else:\n",
    "                    model = MultinomialNB(alpha=alpha, fit_prior=fit_prior)\n",
    "\n",
    "                model.fit(train, train_cols)\n",
    "                temp = pd.DataFrame(model.predict(valid)).astype(int)\n",
    "                new_f1 = f1_score(valid_cols.astype(int), temp, average='weighted')\n",
    "\n",
    "                if output_f1 < new_f1:\n",
    "                    output_f1 = new_f1\n",
    "                    output_model = model\n",
    "    \n",
    "    #when the model is a decision tree\n",
    "    if model_used == \"DecisionTree\":\n",
    "        for criteria in parameters.get(\"criteria\"):\n",
    "            for splitter in parameters.get(\"splitter\"):\n",
    "                for class_weight in parameters.get(\"class_weight\"):\n",
    "                    for min_samples_split in parameters.get(\"min_samples_split\"):\n",
    "                        for min_samples_leaf in parameters.get(\"min_samples_leaf\"):\n",
    "                            for max_depth in parameters.get(\"max_depth\"):\n",
    "                                model = DecisionTreeClassifier(class_weight=class_weight, criterion=criteria,\n",
    "                                                               splitter=splitter, min_samples_leaf=min_samples_leaf,\n",
    "                                                               min_impurity_split=min_samples_split, max_depth=max_depth)\n",
    "                                model.fit(train, train_cols)\n",
    "                                temp = pd.DataFrame(model.predict(valid)).astype(int)\n",
    "                                new_f1 = f1_score(valid_cols.astype(int), temp, average='weighted')\n",
    "\n",
    "                                if output_f1 < new_f1:\n",
    "                                    output_f1 = new_f1\n",
    "                                    output_model = model\n",
    "\n",
    "    #when the model is a linear support vector machine\n",
    "    if model_used == \"LinearSVC\":\n",
    "            for penalty in parameters.get(\"penalty\"):\n",
    "                for loss in parameters.get(\"loss\"):\n",
    "                    # Necessary because the combination of l1 loss and a penalty is not doable\n",
    "                    if penalty == 'l1' and (loss == 'hinge' or loss == 'squared_hinge'):\n",
    "                        continue\n",
    "                    for tol in parameters.get(\"tol\"):\n",
    "                        for C in parameters.get(\"C\"):\n",
    "                            for max_iter in parameters.get(\"max_iter\"):\n",
    "                                for class_weight in parameters.get(\"class_weight\"):\n",
    "                                    model = LinearSVC(penalty=penalty, loss=loss, tol=tol, C=C, max_iter=max_iter,\n",
    "                                                      class_weight=class_weight)\n",
    "                                    model.fit(train, train_cols)\n",
    "                                    temp = pd.DataFrame(model.predict(valid)).astype(int)\n",
    "                                    new_f1 = f1_score(valid_cols.astype(int), temp, average='weighted')\n",
    "\n",
    "                                    if output_f1 < new_f1:\n",
    "                                        output_f1 = new_f1\n",
    "                                        output_model = model\n",
    " \n",
    "    print (output_model)\n",
    "    print(\"_________________________________________\")   \n",
    "    prediction = pd.DataFrame(output_model.predict(valid)).astype(int)\n",
    "    print(\"F1 Score(Valid): \", f1_score(valid_cols.astype(int), prediction, average='micro'))\n",
    "    print(\"\\n\")\n",
    "    return output_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Model Training for  Yelp Review(Binary Bag of Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As the assignment specified, many models with different paramters, i ran uniform random classifier, and majority classifier, naive bayes, decision tree, and finally linear support vector machine, we do this for yelp binary bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyClassifier(constant=None, random_state=None, strategy='uniform')\n",
      "_________________________________________\n",
      "F1 Score(Train):  0.20614285714285713\n",
      "F1 Score(Valid):  0.209\n",
      "F1 Score(Test):  0.19850000000000004\n",
      "\n",
      "\n",
      "DummyClassifier(constant=None, random_state=None, strategy='most_frequent')\n",
      "_________________________________________\n",
      "F1 Score(Train):  0.3525714285714286\n",
      "F1 Score(Valid):  0.356\n",
      "F1 Score(Test):  0.351\n",
      "\n",
      "\n",
      "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=False)\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.42999999999999994\n",
      "\n",
      "\n",
      "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=False)\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.42999999999999994\n",
      "\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=9,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=2,\n",
      "            min_samples_leaf=6, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.399\n",
      "\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=18,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=2,\n",
      "            min_samples_leaf=14, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.399\n",
      "\n",
      "\n",
      "LinearSVC(C=0.001, class_weight='balanced', dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=1.0,\n",
      "     verbose=0)\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.51\n",
      "\n",
      "\n",
      "LinearSVC(C=0.0043, class_weight='balanced', dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=10000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=3, verbose=0)\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.52\n",
      "\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Uniform Random Classifier\n",
    "yelp_uniform_random_classifier = DummyClassifier(strategy='uniform')\n",
    "yelp_uniform_random_classifier = model_training(yelp_uniform_random_classifier, binary_yelp_train, yelp_train[1], binary_yelp_valid,\n",
    "              yelp_valid[1], binary_yelp_test, yelp_test[1])\n",
    "\n",
    "\n",
    "#Majority Class Classifier\n",
    "yelp_majority_classifier = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "yelp_majority_classifier = model_training(yelp_majority_classifier, binary_yelp_train, yelp_train[1], binary_yelp_valid,\n",
    "              yelp_valid[1], binary_yelp_test, yelp_test[1])\n",
    "\n",
    "\n",
    "#Naive Bayes with hyperparameters\n",
    "naive_bayes_params = dict(alpha=np.array([100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]),\n",
    "                          fit_prior=[True, False])\n",
    "\n",
    "yelp_binary_NBC = model_training(None, binary_yelp_train, yelp_train.loc[:, [1]], binary_yelp_valid,\n",
    "              yelp_valid.loc[:, [1]], binary_yelp_test, yelp_test.loc[:, [1]],\n",
    "              parameters=naive_bayes_params, model_used=\"BernoulliNaiveBayes\")\n",
    "\n",
    "\n",
    "\n",
    "reduced_naive_bayes_params = dict(alpha=np.arange(0.002, 0.099, 0.001), fit_prior=[False])\n",
    "\n",
    "yelp_binary_NBC = model_training(None, binary_yelp_train, yelp_train.loc[:, [1]], binary_yelp_valid,\n",
    "              yelp_valid.loc[:, [1]], binary_yelp_test, yelp_test.loc[:, [1]],\n",
    "              parameters=reduced_naive_bayes_params, model_used=\"BernoulliNaiveBayes\")\n",
    "\n",
    "\n",
    "#Decision Tree with hyperparameters\n",
    "tree_params = dict(criteria=['gini', 'entropy'], splitter=['best', 'random'], max_depth=list(range(1, 10)),\n",
    "                            min_samples_split= list(range(2, 10)), min_samples_leaf=list(range(1, 10)), class_weight=[\"balanced\", None])\n",
    "\n",
    "yelp_binary_DTC = model_training(None, binary_yelp_train, yelp_train.loc[:, [1]], binary_yelp_valid,\n",
    "              yelp_valid.loc[:, [1]], binary_yelp_test, yelp_test.loc[:, [1]],\n",
    "              parameters=tree_params, model_used=\"DecisionTree\")\n",
    "\n",
    "\n",
    "reduced_tree_params = dict(criteria=['entropy'], splitter=['best'], max_depth=list(range(1, 30)),\n",
    "                            min_samples_split= list(range(2, 20)), min_samples_leaf=list(range(1, 20)), class_weight=[None])\n",
    "\n",
    "yelp_binary_DTC = model_training(None, binary_yelp_train, yelp_train.loc[:, [1]], binary_yelp_valid,\n",
    "              yelp_valid.loc[:, [1]], binary_yelp_test, yelp_test.loc[:, [1]], \n",
    "              parameters=reduced_tree_params, model_used=\"DecisionTree\")\n",
    "\n",
    "\n",
    "\n",
    "#Linear Support Vector Machine\n",
    "linear_SVC_params = dict(penalty=['l1', 'l2'], loss=['hinge', 'squared_hinge'], class_weight=[None, 'balanced'],\n",
    "                         tol=np.array([1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.0000001, 0.00000001]), max_iter =[1000],\n",
    "                         C=([1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001,]))\n",
    "\n",
    "yelp_binary_LSVC = model_training(None, binary_yelp_train, yelp_train.loc[:, [1]], binary_yelp_valid,\n",
    "              yelp_valid.loc[:, [1]], binary_yelp_test, yelp_test.loc[:, [1]],\n",
    "              parameters= linear_SVC_params, model_used=\"LinearSVC\")\n",
    "\n",
    "\n",
    "\n",
    "reduced_linear_SVC_params = dict(penalty=['l2'], loss=['squared_hinge'], class_weight=['balanced'],\n",
    "                         tol=np.array([3, 2, 1]), max_iter=[1000, 10000], C=np.arange(0.0002, 0.0099, 0.0001))\n",
    "\n",
    "yelp_binary_LSVC = model_training(None, binary_yelp_train, yelp_train.loc[:, [1]], binary_yelp_valid,\n",
    "              yelp_valid.loc[:, [1]], binary_yelp_test, yelp_test.loc[:, [1]],\n",
    "              parameters= reduced_linear_SVC_params, model_used=\"LinearSVC\")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Model Training for  Yelp Review(Frequency Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=False)\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.508\n",
      "\n",
      "\n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=False)\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.508\n",
      "\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=9,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=2,\n",
      "            min_samples_leaf=7, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.40599999999999997\n",
      "\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=19,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=2,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.3970000000000001\n",
      "\n",
      "\n",
      "LinearSVC(C=10, class_weight='balanced', dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.001,\n",
      "     verbose=0)\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.504\n",
      "\n",
      "\n",
      "LinearSVC(C=0.0048000000000000004, class_weight='balanced', dual=True,\n",
      "     fit_intercept=True, intercept_scaling=1, loss='squared_hinge',\n",
      "     max_iter=10000, multi_class='ovr', penalty='l2', random_state=None,\n",
      "     tol=2, verbose=0)\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.374\n",
      "\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Naive Bayes with hyperparameters\n",
    "naive_bayes_params = dict(alpha=np.array([100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]),\n",
    "                          fit_prior=[True, False])\n",
    "\n",
    "yelp_freq_NBC = model_training(None, frequency_yelp_train, yelp_train.loc[:, [1]], frequency_yelp_valid,\n",
    "              yelp_valid.loc[:, [1]], frequency_yelp_test, yelp_test.loc[:, [1]],\n",
    "              parameters=naive_bayes_params, model_used=\"MultinomialNaiveBayes\")\n",
    "\n",
    "\n",
    "\n",
    "reduced_naive_bayes_params = dict(alpha=np.arange(0.002, 0.099, 0.001), fit_prior=[False])\n",
    "\n",
    "yelp_freq_NBC = model_training(None, frequency_yelp_train, yelp_train.loc[:, [1]], frequency_yelp_valid,\n",
    "              yelp_valid.loc[:, [1]], frequency_yelp_test, yelp_test.loc[:, [1]],\n",
    "              parameters=reduced_naive_bayes_params, model_used=\"MultinomialNaiveBayes\")\n",
    "\n",
    "\n",
    "#Decision Tree with hyperparameters\n",
    "tree_params = dict(criteria=['gini', 'entropy'], splitter=['best', 'random'], max_depth=list(range(1, 10)),\n",
    "                            min_samples_split= list(range(2, 10)), min_samples_leaf=list(range(1, 10)), class_weight=[\"balanced\", None])\n",
    "\n",
    "yelp_freq_DTC = model_training(None, frequency_yelp_train, yelp_train.loc[:, [1]], frequency_yelp_valid,\n",
    "              yelp_valid.loc[:, [1]], frequency_yelp_test, yelp_test.loc[:, [1]],\n",
    "              parameters=tree_params, model_used=\"DecisionTree\")\n",
    "\n",
    "\n",
    "reduced_tree_params = dict(criteria=['entropy'], splitter=['best'], max_depth=list(range(1, 30)),\n",
    "                            min_samples_split= list(range(2, 20)), min_samples_leaf=list(range(1, 20)), class_weight=[None])\n",
    "\n",
    "yelp_freq_DTC = model_training(None, frequency_yelp_train, yelp_train.loc[:, [1]], frequency_yelp_valid,\n",
    "              yelp_valid.loc[:, [1]], frequency_yelp_test, yelp_test.loc[:, [1]], \n",
    "              parameters=reduced_tree_params, model_used=\"DecisionTree\")\n",
    "\n",
    "\n",
    "\n",
    "#Linear Support Vector Machine\n",
    "linear_SVC_params = dict(penalty=['l1', 'l2'], loss=['hinge', 'squared_hinge'], class_weight=[None, 'balanced'],\n",
    "                         tol=np.array([1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.0000001, 0.00000001]), max_iter =[1000],\n",
    "                         C=([1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001,]))\n",
    "\n",
    "yelp_freq_LSVC = model_training(None, frequency_yelp_train, yelp_train.loc[:, [1]], frequency_yelp_valid,\n",
    "              yelp_valid.loc[:, [1]], frequency_yelp_test, yelp_test.loc[:, [1]],\n",
    "              parameters= linear_SVC_params, model_used=\"LinearSVC\")\n",
    "\n",
    "\n",
    "\n",
    "reduced_linear_SVC_params = dict(penalty=['l2'], loss=['squared_hinge'], class_weight=['balanced'],\n",
    "                         tol=np.array([3, 2, 1]), max_iter=[1000, 10000], C=np.arange(0.0002, 0.0099, 0.0001))\n",
    "\n",
    "yelp_freq_LSVC = model_training(None, frequency_yelp_train, yelp_train.loc[:, [1]], frequency_yelp_valid,\n",
    "              yelp_valid.loc[:, [1]], frequency_yelp_test, yelp_test.loc[:, [1]],\n",
    "              parameters= reduced_linear_SVC_params, model_used=\"LinearSVC\")\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Model Training for  IMDB Review(Binary Bag of Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below code initalizes all the datasets used and creates vocabulary list and tranlsates text files for IMDB reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "IMDB_train=pd.read_csv(\"IMDB-train.txt\", sep='\\t', header=None)\n",
    "IMDB_test=pd.read_csv(\"IMDB-test.txt\", sep='\\t', header=None)\n",
    "IMDB_valid=pd.read_csv(\"IMDB-valid.txt\", sep='\\t', header=None)\n",
    "\n",
    "\n",
    "IMDB_vocab = vocabulary_list(IMDB_train,10000)\n",
    "IMDB_vocab_file = makeFile(\"IMDB-vocab.txt\",IMDB_vocab)\n",
    "\n",
    "makeTranslation(IMDB_train,\"IMDB-vocab.txt\",\"IMDB-train.txt\",IMDB_vocab)\n",
    "makeTranslation(IMDB_test,\"IMDB-vocab.txt\",\"IMDB-test.txt\",IMDB_vocab)\n",
    "makeTranslation(IMDB_valid,\"IMDB-vocab.txt\",\"IMDB-valid.txt\",IMDB_vocab)\n",
    "\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "IMDB_train=pd.read_csv(\"IMDB-train.txt\", sep='\\t', header=None, encoding='ISO-8859-1', dtype=None)\n",
    "IMDB_test=pd.read_csv(\"IMDB-test.txt\", sep='\\t', header=None, encoding='ISO-8859-1', dtype=None)\n",
    "IMDB_valid=pd.read_csv(\"IMDB-valid.txt\", sep='\\t', header=None, encoding='ISO-8859-1', dtype=None)\n",
    "\n",
    "binary_IMDB_train=binary_bag_of_words(IMDB_train,10000)\n",
    "binary_IMDB_test=binary_bag_of_words(IMDB_test,10000)\n",
    "binary_IMDB_valid=binary_bag_of_words(IMDB_valid,10000)\n",
    "\n",
    "\n",
    "frequency_IMDB_train=frequency_bag_of_words(IMDB_train,10000)\n",
    "frequency_IMDB_test=frequency_bag_of_words(IMDB_test,10000)\n",
    "frequency_IMDB_valid=frequency_bag_of_words(IMDB_valid,10000)\n",
    "\n",
    "\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyClassifier(constant=None, random_state=None, strategy='uniform')\n",
      "_________________________________________\n",
      "F1 Score(Train):  0.5044666666666666\n",
      "F1 Score(Valid):  0.4939\n",
      "F1 Score(Test):  0.503\n",
      "\n",
      "\n",
      "DummyClassifier(constant=None, random_state=None, strategy='most_frequent')\n",
      "_________________________________________\n",
      "F1 Score(Train):  0.5\n",
      "F1 Score(Valid):  0.5\n",
      "F1 Score(Test):  0.5\n",
      "\n",
      "\n",
      "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.8432\n",
      "\n",
      "\n",
      "BernoulliNB(alpha=0.014, binarize=0.0, class_prior=None, fit_prior=False)\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.8434\n",
      "\n",
      "\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=2,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.5\n",
      "\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=2,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.5\n",
      "\n",
      "\n",
      "LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=1.0,\n",
      "     verbose=0)\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.8769999999999999\n",
      "\n",
      "\n",
      "LinearSVC(C=0.005700000000000001, class_weight='balanced', dual=True,\n",
      "     fit_intercept=True, intercept_scaling=1, loss='squared_hinge',\n",
      "     max_iter=1000, multi_class='ovr', penalty='l2', random_state=None,\n",
      "     tol=3, verbose=0)\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.8793\n",
      "\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Uniform Random Classifier\n",
    "IMDB_uniform_random_classifier = DummyClassifier(strategy='uniform')\n",
    "IMDB_uniform_random_classifier = model_training(IMDB_uniform_random_classifier, binary_IMDB_train, IMDB_train[1], binary_IMDB_valid,\n",
    "              IMDB_valid[1], binary_IMDB_test, IMDB_test[1])\n",
    "\n",
    "\n",
    "#Majority Class Classifier\n",
    "IMDB_majority_classifier = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "IMDB_majority_classifier = model_training(IMDB_majority_classifier, binary_IMDB_train, IMDB_train[1], binary_IMDB_valid,\n",
    "              IMDB_valid[1], binary_IMDB_test, IMDB_test[1])\n",
    "\n",
    "\n",
    "#Naive Bayes with hyperparameters\n",
    "naive_bayes_params = dict(alpha=np.array([100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]),\n",
    "                          fit_prior=[True, False])\n",
    "\n",
    "IMDB_binary_NBC = model_training(None, binary_IMDB_train, IMDB_train.loc[:, [1]], binary_IMDB_valid,\n",
    "              IMDB_valid.loc[:, [1]], binary_IMDB_test, IMDB_test.loc[:, [1]],\n",
    "              parameters=naive_bayes_params, model_used=\"BernoulliNaiveBayes\")\n",
    "\n",
    "\n",
    "\n",
    "reduced_naive_bayes_params = dict(alpha=np.arange(0.002, 0.099, 0.001), fit_prior=[False])\n",
    "\n",
    "IMDB_binary_NBC = model_training(None, binary_IMDB_train, IMDB_train.loc[:, [1]], binary_IMDB_valid,\n",
    "              IMDB_valid.loc[:, [1]], binary_IMDB_test, IMDB_test.loc[:, [1]],\n",
    "              parameters=reduced_naive_bayes_params, model_used=\"BernoulliNaiveBayes\")\n",
    "\n",
    "\n",
    "#Decision Tree with hyperparameters\n",
    "tree_params = dict(criteria=['gini', 'entropy'], splitter=['best', 'random'], max_depth=list(range(1, 10)),\n",
    "                            min_samples_split= list(range(2, 10)), min_samples_leaf=list(range(1, 10)), class_weight=[\"balanced\", None])\n",
    "\n",
    "IMDB_binary_DTC = model_training(None, binary_IMDB_train, IMDB_train.loc[:, [1]], binary_IMDB_valid,\n",
    "              IMDB_valid.loc[:, [1]], binary_IMDB_test, IMDB_test.loc[:, [1]],\n",
    "              parameters=tree_params, model_used=\"DecisionTree\")\n",
    "\n",
    "\n",
    "reduced_tree_params = dict(criteria=['entropy'], splitter=['best'], max_depth=list(range(1, 30)),\n",
    "                            min_samples_split= list(range(2, 20)), min_samples_leaf=list(range(1, 20)), class_weight=[None])\n",
    "\n",
    "IMDB_binary_DTC = model_training(None, binary_IMDB_train, IMDB_train.loc[:, [1]], binary_IMDB_valid,\n",
    "              IMDB_valid.loc[:, [1]], binary_IMDB_test, IMDB_test.loc[:, [1]], \n",
    "              parameters=reduced_tree_params, model_used=\"DecisionTree\")\n",
    "\n",
    "\n",
    "\n",
    "#Linear Support Vector Machine\n",
    "linear_SVC_params = dict(penalty=['l1', 'l2'], loss=['hinge', 'squared_hinge'], class_weight=[None, 'balanced'],\n",
    "                         tol=np.array([1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.0000001, 0.00000001]), max_iter =[1000],\n",
    "                         C=([1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.0000001, 0.00000001]))\n",
    "\n",
    "IMDB_binary_LSVC = model_training(None, binary_IMDB_train, IMDB_train.loc[:, [1]], binary_IMDB_valid,\n",
    "              IMDB_valid.loc[:, [1]], binary_IMDB_test, IMDB_test.loc[:, [1]],\n",
    "              parameters= linear_SVC_params, model_used=\"LinearSVC\")\n",
    "\n",
    "\n",
    "\n",
    "reduced_linear_SVC_params = dict(penalty=['l2'], loss=['squared_hinge'], class_weight=['balanced'],\n",
    "                         tol=np.array([3, 2, 1]), max_iter=[1000, 10000], C=np.arange(0.0002, 0.0099, 0.0001))\n",
    "\n",
    "IMDB_binary_LSVC = model_training(None, binary_IMDB_train, IMDB_train.loc[:, [1]], binary_IMDB_valid,\n",
    "              IMDB_valid.loc[:, [1]], binary_IMDB_test, IMDB_test.loc[:, [1]],\n",
    "              parameters= reduced_linear_SVC_params, model_used=\"LinearSVC\")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Model Training for  IMDB Review(Frequency Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=0.0001, class_prior=None, fit_prior=True)\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.8495\n",
      "\n",
      "\n",
      "MultinomialNB(alpha=0.002, class_prior=None, fit_prior=False)\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.8496\n",
      "\n",
      "\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=2,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.5\n",
      "\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=2,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.5\n",
      "\n",
      "\n",
      "LinearSVC(C=100, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.8804\n",
      "\n",
      "\n",
      "LinearSVC(C=0.006400000000000001, class_weight='balanced', dual=True,\n",
      "     fit_intercept=True, intercept_scaling=1, loss='squared_hinge',\n",
      "     max_iter=1000, multi_class='ovr', penalty='l2', random_state=None,\n",
      "     tol=2, verbose=0)\n",
      "_________________________________________\n",
      "F1 Score(Valid):  0.652\n",
      "\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Naive Bayes with hyperparameters\n",
    "naive_bayes_params = dict(alpha=np.array([100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]),\n",
    "                          fit_prior=[True, False])\n",
    "\n",
    "IMDB_freq_NBC = model_training(None, frequency_IMDB_train, IMDB_train.loc[:, [1]], frequency_IMDB_valid,\n",
    "              IMDB_valid.loc[:, [1]], frequency_IMDB_test, IMDB_test.loc[:, [1]],\n",
    "              parameters=naive_bayes_params, model_used=\"MultinomialNaiveBayes\")\n",
    "\n",
    "\n",
    "\n",
    "reduced_naive_bayes_params = dict(alpha=np.arange(0.002, 0.099, 0.001), fit_prior=[False])\n",
    "\n",
    "\n",
    "IMDB_freq_NBC = model_training(None, frequency_IMDB_train, IMDB_train.loc[:, [1]], frequency_IMDB_valid,\n",
    "              IMDB_valid.loc[:, [1]], frequency_IMDB_test, IMDB_test.loc[:, [1]],\n",
    "              parameters=reduced_naive_bayes_params, model_used=\"MultinomialNaiveBayes\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Decision Tree with hyperparameters\n",
    "tree_params = dict(criteria=['gini', 'entropy'], splitter=['best', 'random'], max_depth=list(range(1, 10)),\n",
    "                            min_samples_split= list(range(2, 10)), min_samples_leaf=list(range(1, 10)), class_weight=[\"balanced\", None])\n",
    "\n",
    "IMDB_freq_DTC = model_training(None, frequency_IMDB_train, IMDB_train.loc[:, [1]], frequency_IMDB_valid,\n",
    "              IMDB_valid.loc[:, [1]], frequency_IMDB_test, IMDB_test.loc[:, [1]],\n",
    "              parameters=tree_params, model_used=\"DecisionTree\")\n",
    "\n",
    "\n",
    "reduced_tree_params = dict(criteria=['entropy'], splitter=['best'], max_depth=list(range(1, 30)),\n",
    "                            min_samples_split= list(range(2, 20)), min_samples_leaf=list(range(1, 20)), class_weight=[None])\n",
    "\n",
    "IMDB_freq_DTC = model_training(None, frequency_IMDB_train, IMDB_train.loc[:, [1]], frequency_IMDB_valid,\n",
    "              IMDB_valid.loc[:, [1]], frequency_IMDB_test, IMDB_test.loc[:, [1]], \n",
    "              parameters=reduced_tree_params, model_used=\"DecisionTree\")\n",
    "\n",
    "\n",
    "\n",
    "#Linear Support Vector Machine\n",
    "linear_SVC_params = dict(penalty=['l1', 'l2'], loss=['hinge', 'squared_hinge'], class_weight=[None, 'balanced'],\n",
    "                         tol=np.array([1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.0000001, 0.00000001]), max_iter =[1000],\n",
    "                         C=([1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.0000001, 0.00000001]))\n",
    "\n",
    "IMDB_freq_LSVC = model_training(None, frequency_IMDB_train, IMDB_train.loc[:, [1]], frequency_IMDB_valid,\n",
    "              IMDB_valid.loc[:, [1]], frequency_IMDB_test, IMDB_test.loc[:, [1]],\n",
    "              parameters= linear_SVC_params, model_used=\"LinearSVC\")\n",
    "\n",
    "\n",
    "\n",
    "reduced_linear_SVC_params = dict(penalty=['l2'], loss=['squared_hinge'], class_weight=['balanced'],\n",
    "                         tol=np.array([3, 2, 1]), max_iter=[1000, 10000], C=np.arange(0.0002, 0.0099, 0.0001))\n",
    "\n",
    "IMDB_freq_LSVC = model_training(None, frequency_IMDB_train, IMDB_train.loc[:, [1]], frequency_IMDB_valid,\n",
    "              IMDB_valid.loc[:, [1]], frequency_IMDB_test, IMDB_test.loc[:, [1]],\n",
    "              parameters= reduced_linear_SVC_params, model_used=\"LinearSVC\")\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
