{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comp 551-Assignment 2\n",
    "\n",
    "\n",
    "## Aymen Rumi-260661663\n",
    "   \n",
    "### ***-Refer to report for further detailed exlanation/observation/answers to each inidivudal question and their subquestions: this file contains mainly code and viualizations with explanation for my work, this file is still very comprehensive of the assignment-***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all modules needed for assignment\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "\n",
    "# Better visualization for plots\n",
    "plt.rcParams['figure.figsize'] = (20.0, 20.0)\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to import data given text file, adjusts to array dimensions and removes nan values\n",
    "def getData(textFile):\n",
    "    file=np.genfromtxt(textFile, delimiter=',')\n",
    "    if(len(np.shape(file))==1):\n",
    "        file = [x for x in file if str(x) != 'nan'] \n",
    "    else:\n",
    "        cutoff=np.shape(file)[1]-1\n",
    "        file = file[0:cutoff,0:cutoff]  \n",
    "    return file\n",
    "\n",
    "\n",
    "# function to generate multivariate data distribution with label, given mean vector, covariance matrix, and amount of data points\n",
    "def generateMultiVariableData(sign,mean,covariance,amount):\n",
    "    if(sign==-1):\n",
    "        label='Negative'\n",
    "        \n",
    "    if(sign==1):\n",
    "        label='Positive'\n",
    "        \n",
    "    data=[]\n",
    "    for i in range(amount+1):\n",
    "        data.append((label,np.random.multivariate_normal(mean, covariance)))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# function to split data classes into specified amount for train,validate, and test, and returns them\n",
    "def splitData(test,validation,train,class1,class2):\n",
    "    np.random.shuffle(class1)\n",
    "    np.random.shuffle(class2)\n",
    "\n",
    "    test_set = [i for i in class1[0:int((len(class1)*test))]] + [j for j in class2[0:int((len(class2)*test))]]\n",
    "    np.random.shuffle(test_set)\n",
    "    pd.DataFrame(test_set).to_csv('DS1_test') \n",
    "    validation_set = [x for x in class1[int((len(class1)*test)):int(((len(class1)*test)+(len(class1)*validation)))]] + [y for y in class2[int((len(class2)*test)):int(((len(class2)*test)+(len(class2)*validation)))]]\n",
    "    np.random.shuffle(validation_set)\n",
    "    pd.DataFrame(validation_set).to_csv('DS1_valid')  \n",
    "    class1 = class1[int(((len(class1)*test)+(len(class1)*validation))):len(class1)]    \n",
    "    class2 = class2[int(((len(class2)*test)+(len(class2)*validation))):len(class2)]   \n",
    "    train_set = class1 + class2\n",
    "    np.random.shuffle(train_set)\n",
    "    \n",
    "    pd.DataFrame(train_set).to_csv('DS1_train')\n",
    "    \n",
    "    return[test_set,validation_set,train_set]\n",
    "    \n",
    "\n",
    "# initializing mean vectors for positive and negative classification, and covariance matrix\n",
    "meanVector_negative = getData('DS1_m_1.txt')\n",
    "meanVector_positive=getData('DS1_m_0.txt')\n",
    "covariance_matrix = getData('DS1_Cov.txt')\n",
    "\n",
    "\n",
    "# initializing seperation class data\n",
    "negative_class = generateMultiVariableData(-1,meanVector_negative,covariance_matrix,2000)\n",
    "positive_class = generateMultiVariableData(1,meanVector_positive,covariance_matrix,2000)\n",
    "\n",
    "\n",
    "#initializing all dataset\n",
    "DS1 = positive_class + negative_class\n",
    "np.random.shuffle(DS1)\n",
    "pd.DataFrame(DS1).to_csv('DS1')\n",
    "\n",
    "\n",
    "#intitializing test, validation, and trainingset\n",
    "test_set=splitData(.2,.2,.6,negative_class,positive_class)[0]\n",
    "validation_set=splitData(.2,.2,.6,negative_class,positive_class)[1]\n",
    "train_set=splitData(.2,.2,.6,negative_class,positive_class)[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Gaussian Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: =  0.96\n",
      "Precision: =  0.9670050761421319\n",
      "Recall:  0.9525\n",
      "F-measure:  0.9596977329974811\n",
      "Weights learned: W= [[ 14.1902841 ]\n",
      " [ -8.29689286]\n",
      " [ -5.01320057]\n",
      " [ -2.63225818]\n",
      " [ -9.24777763]\n",
      " [ -4.32177611]\n",
      " [ 15.67718196]\n",
      " [-23.88023661]\n",
      " [-28.18375148]\n",
      " [  9.21204196]\n",
      " [-12.8650161 ]\n",
      " [-11.47585923]\n",
      " [ 14.95966034]\n",
      " [ 12.30936621]\n",
      " [ -5.82786657]\n",
      " [ 12.99760802]\n",
      " [ 28.04494654]\n",
      " [ -6.78545758]\n",
      " [  0.13021411]\n",
      " [ -5.14591346]]\n",
      "W0= [[26.6463063]]\n"
     ]
    }
   ],
   "source": [
    "# method to find phi given both classes\n",
    "def Phi(positive_class,negative_class):\n",
    "    return len(positive_class)/len(positive_class+negative_class)\n",
    "\n",
    "#find mean matrix given class data\n",
    "def meanMatrix(class_data):\n",
    "    mean=[]\n",
    "    for i in range(len(class_data[0][1])):\n",
    "        value=0\n",
    "        for data_point in class_data:\n",
    "            value =value+ data_point[1][i]\n",
    "        mean.append(value/len(class_data))\n",
    "    return np.matrix(mean).transpose()\n",
    "  \n",
    "#find s value to plug into covariance function\n",
    "def sValue(class_data):\n",
    "    s = 0.0\n",
    "    mean_matrix=meanMatrix(class_data)   \n",
    "    for point in class_data:\n",
    "        matrix = np.matrix(point[1]).transpose()\n",
    "        matrix2 = (matrix - mean_matrix) * (matrix - mean_matrix).transpose()\n",
    "        s= s+matrix2\n",
    "    s = s/len(class_data)\n",
    "    return s\n",
    "\n",
    "#finding covariance matrix, given both classes and s\n",
    "def covarianceMatrix(class1,s1,class2,s2):\n",
    "    return ((len(class1)/len(class1 + class2))*s1) + ((len(class2)/len(class1 + class2))*s2)\n",
    "\n",
    "#finding coeffecients for w and w0\n",
    "def coefficientMatrices(phi,mean0,mean1,covariance):\n",
    "    w = (covariance**-1)*(mean0 - mean1)\n",
    "    w_0 = -0.5*(mean0.transpose()*(covariance**-1)*mean0) + 0.5*(mean1.transpose()*(covariance**-1)*mean1) + math.log(phi/(1-phi))   \n",
    "    return[w,w_0]\n",
    "\n",
    "#method given a data point, gives a value between 0 and 1 for the classification\n",
    "def GDA_classification(data_point,w,w0):\n",
    "    matrix = np.matrix(data_point).transpose()\n",
    "    y = w.transpose()*matrix + w0\n",
    "    return 1/(1 + math.exp(-y))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# initializing variables\n",
    "phi=Phi(positive_class,negative_class)\n",
    "mean0=meanMatrix(negative_class)\n",
    "mean1=meanMatrix(positive_class)\n",
    "s0=sValue(negative_class)\n",
    "s1=sValue(positive_class)\n",
    "covariance=covarianceMatrix(positive_class,s0,negative_class,s1)\n",
    "\n",
    "\n",
    "#finding weights\n",
    "w=coefficientMatrices(phi,mean1,mean0,covariance)[0]\n",
    "w0=coefficientMatrices(phi,mean1,mean0,covariance)[1]\n",
    "\n",
    "\n",
    "# method given dataset, and weights, gives all the classification evaluations, such as accuracy, precision, and f measure, wtc, wtc\n",
    "def classifier_evaluation(dataset,w,w0):\n",
    "    \n",
    "    #initializing all measurement citeria\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    true_pos = 0\n",
    "    true_neg = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    for point in dataset:\n",
    "        if GDA_classification(point[1],w,w0) > 0.5 :\n",
    "        \n",
    "            if point[0] == 'Positive':\n",
    "                correct += 1\n",
    "                true_pos += 1\n",
    "            else:\n",
    "                incorrect += 1\n",
    "                false_pos += 1\n",
    "        else:\n",
    "\n",
    "            if point[0] == 'Negative':\n",
    "                correct += 1\n",
    "                true_neg += 1\n",
    "            else:\n",
    "                incorrect += 1\n",
    "                false_neg += 1\n",
    "    \n",
    "\n",
    "    accuracy = correct / (correct + incorrect)\n",
    "    precision = true_pos/(true_pos + false_pos)\n",
    "    recall = true_pos/(true_pos + false_neg)\n",
    "    F_measure = (2*precision*recall) / (precision + recall)\n",
    "\n",
    "    # Reporting the coefficient matrix learned by the classifier and the results\n",
    "    print(\"Accuracy: = \", accuracy)\n",
    "    print(\"Precision: = \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F-measure: \", F_measure)\n",
    "    print(\"Weights learned: W=\", w)\n",
    "    print(\"W0=\", w0)\n",
    "\n",
    "\n",
    "#calling method on training set\n",
    "classifier_evaluation(test_set,w,w0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# method for classification, given both classes and k, \n",
    "def KNN_classification(tag,k,class1,class2):\n",
    "    distances=[]\n",
    "    data=class1+class2\n",
    "    \n",
    "    for data_points in data:\n",
    "        distances.append((abs(euclidean(tag, data_points[1])), data_points[0]))\n",
    "        \n",
    "    distances.sort()\n",
    "    \n",
    "    class1_count=0\n",
    "    class2_count=0\n",
    "    \n",
    "    for i in range(k):\n",
    "        if distances[i][1]==class1[0][0]:\n",
    "            class1_count +=1\n",
    "        else:\n",
    "            class2_count +=1\n",
    "            \n",
    "    if class1_count == class2_count:\n",
    "\n",
    "            if random.random() <= 0.5:\n",
    "\n",
    "                return class1[0][0]\n",
    "\n",
    "            else:\n",
    "\n",
    "                return class2[0][0]\n",
    "\n",
    "    elif class1_count > class2_count:\n",
    "\n",
    "            return class1[0][0]\n",
    "\n",
    "    else:\n",
    "\n",
    "            return class2[0][0]\n",
    "\n",
    "\n",
    "# method for evaluation for each KNN given amount of k\n",
    "def KNN_Evaluation(amount):       \n",
    "    ks = [x for x in range(1, amount)]\n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    F_measure_list = []\n",
    "\n",
    "    # for loop to go through all the ks\n",
    "    for k in ks:\n",
    "        \n",
    "        #initializse evaluation criterion\n",
    "        correct= 0\n",
    "        incorrect = 0\n",
    "        true_pos = 0\n",
    "        true_neg = 0\n",
    "        false_pos = 0\n",
    "        false_neg = 0\n",
    "\n",
    "        for data_point in test_set:\n",
    "            predicted_class = KNN_classification(data_point[1], k,positive_class,negative_class)\n",
    "            if predicted_class == \"Positive\":\n",
    "                if data_point[0] == \"Positive\":\n",
    "                    correct += 1\n",
    "                    true_pos += 1\n",
    "                else:\n",
    "                    incorrect += 1\n",
    "                    false_pos += 1\n",
    "            else:\n",
    "                if data_point[0] == \"Negative\":\n",
    "                    correct += 1\n",
    "                    true_neg += 1\n",
    "                else:\n",
    "                    incorrect += 1\n",
    "                    false_neg += 1\n",
    "\n",
    "        accuracy_list.append(correct / (correct + incorrect))\n",
    "        precision = true_pos/(true_pos + false_pos)\n",
    "        precision_list.append(precision)\n",
    "        recall = true_pos/(true_pos + false_neg)\n",
    "        recall_list.append(recall)\n",
    "        F_measure_list.append((2*precision*recall) / (precision + recall))\n",
    "\n",
    "\n",
    "KNN_Evaluation(20) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Generate Data Part  2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all datasets needed\n",
    "DS2_c1_m1=getData('DS2_c1_m1.txt')\n",
    "DS2_c1_m2=getData('DS2_c1_m2.txt')\n",
    "DS2_c1_m3=getData('DS2_c1_m3.txt')\n",
    "DS2_c2_m1=getData('DS2_c2_m1.txt')\n",
    "DS2_c2_m2=getData('DS2_c2_m2.txt')\n",
    "DS2_c2_m3=getData('DS2_c2_m3.txt')\n",
    "DS2_Cov1=getData('DS2_Cov1.txt')\n",
    "DS2_Cov2=getData('DS2_Cov2.txt')\n",
    "DS2_Cov3=getData('DS2_Cov3.txt')\n",
    "\n",
    "\n",
    "#using same function from q2 but for new dataset\n",
    "def splitData2(test,validation,train,class1,class2):\n",
    "    np.random.shuffle(class1)\n",
    "    np.random.shuffle(class2)\n",
    "\n",
    "    test_set = [i for i in class1[0:int((len(class1)*test))]] + [j for j in class2[0:int((len(class2)*test))]]\n",
    "    np.random.shuffle(test_set)\n",
    "    pd.DataFrame(test_set).to_csv('DS2_test') \n",
    "    validation_set = [x for x in class1[int((len(class1)*test)):int(((len(class1)*test)+(len(class1)*validation)))]] + [y for y in class2[int((len(class2)*test)):int(((len(class2)*test)+(len(class2)*validation)))]]\n",
    "    np.random.shuffle(validation_set)\n",
    "    pd.DataFrame(validation_set).to_csv('DS2_valid')  \n",
    "    class1 = class1[int(((len(class1)*test)+(len(class1)*validation))):len(class1)]    \n",
    "    class2 = class2[int(((len(class2)*test)+(len(class2)*validation))):len(class2)]   \n",
    "    train_set = class1 + class2\n",
    "    np.random.shuffle(train_set)\n",
    "    \n",
    "    pd.DataFrame(train_set).to_csv('DS2_train')\n",
    "    \n",
    "    return[test_set,validation_set,train_set]\n",
    "\n",
    "DS2_negative_class=[]\n",
    "DS2_positive_class=[]\n",
    "\n",
    "for i in range(2001):\n",
    "    prob= random.random()\n",
    "    \n",
    "    if prob <= 0.1:\n",
    "        DS2_negative_class.append(('Negative',np.random.multivariate_normal(DS2_c2_m1,DS2_Cov1)))\n",
    "        DS2_positive_class.append(('Positive',np.random.multivariate_normal(DS2_c1_m1,DS2_Cov1)))\n",
    "    elif prob <= 0.52:\n",
    "        DS2_negative_class.append(('Negative',np.random.multivariate_normal(DS2_c2_m2,DS2_Cov2)))\n",
    "        DS2_positive_class.append(('Positive',np.random.multivariate_normal(DS2_c1_m2,DS2_Cov2)))\n",
    "    else:\n",
    "        DS2_negative_class.append(('Negative',np.random.multivariate_normal(DS2_c2_m3,DS2_Cov3)))\n",
    "        DS2_positive_class.append(('Positive',np.random.multivariate_normal(DS2_c1_m3,DS2_Cov3)))\n",
    "    \n",
    "    \n",
    "np.random.shuffle(DS2_negative_class)\n",
    "np.random.shuffle(DS2_positive_class)\n",
    " \n",
    "DS2=DS2_negative_class+DS2_positive_class\n",
    "np.random.shuffle(DS2)\n",
    "pd.DataFrame(DS2).to_csv('DS2')\n",
    "\n",
    "\n",
    "#intitializing test, validation, and trainingset\n",
    "DS2_test_set=splitData2(.2,.2,.6,DS2_negative_class,DS2_positive_class)[0]\n",
    "DS2_validation_set=splitData2(.2,.2,.6,DS2_negative_class,DS2_positive_class)[1]\n",
    "DS2_train_set=splitData2(.2,.2,.6,DS2_negative_class,DS2_positive_class)[2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Q2 & Q3 with DS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: =  0.54625\n",
      "Precision: =  0.5461346633416458\n",
      "Recall:  0.5475\n",
      "F-measure:  0.5468164794007491\n",
      "Weights learned: W= [[-0.05292645]\n",
      " [ 0.04098204]\n",
      " [-0.02845562]\n",
      " [ 0.01662299]\n",
      " [-0.05905746]\n",
      " [-0.01920297]\n",
      " [-0.02989649]\n",
      " [-0.03463659]\n",
      " [-0.00241258]\n",
      " [ 0.03461814]\n",
      " [-0.00062787]\n",
      " [-0.07470424]\n",
      " [-0.0405562 ]\n",
      " [ 0.03792138]\n",
      " [ 0.04563976]\n",
      " [ 0.00071448]\n",
      " [ 0.04253994]\n",
      " [ 0.02844131]\n",
      " [ 0.0410429 ]\n",
      " [ 0.00287101]]\n",
      "W0= [[0.05923283]]\n"
     ]
    }
   ],
   "source": [
    "DS2_phi=Phi(DS2_positive_class,DS2_negative_class)\n",
    "DS2_mean0=meanMatrix(DS2_negative_class)\n",
    "DS2_mean1=meanMatrix(DS2_positive_class)\n",
    "DS2_s0=sValue(DS2_negative_class)\n",
    "DS2_s1=sValue(DS2_positive_class)\n",
    "DS2_covariance=covarianceMatrix(DS2_positive_class,DS2_s0,DS2_negative_class,DS2_s1)\n",
    "\n",
    "\n",
    "#finding weights\n",
    "D2_w=coefficientMatrices(DS2_phi,DS2_mean1,DS2_mean0,DS2_covariance)[0]\n",
    "D2_w0=coefficientMatrices(DS2_phi,DS2_mean1,DS2_mean0,DS2_covariance)[1]\n",
    "\n",
    "\n",
    "classifier_evaluation(DS2_test_set,D2_w,D2_w0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: DS1 vs DS2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DS1 Dataset seems to contrain less variantion among the data leading to better classifications for GDA, while DS2 seems to be doing less better, this variation is bad for a linear classifier but better for KNN because it accounts of variation within the dataset, this KNN is better for DS2 than DS1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
